/*
 * OpenAI API
 *
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 *
 * API version: 2.0.0
 * Generated by: Swagger Codegen (https://github.com/swagger-api/swagger-codegen.git)
 */
package swagger

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
)

const DefaultModel = "qwen1.5-chat"
const DefaultUrl = "http://172.21.44.125:8091/v1/chat/completions"

func CreateChatCompletion(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json; charset=UTF-8")

	// 从查询参数中获取API密钥
	// apiKey := r.URL.Query().Get("api_key")
	// if apiKey == "" {
	// 	logError(w, "API key is required", http.StatusBadRequest)
	// 	return
	// }
	apiKey := "empty"

	// 解析请求体
	var requestBody CreateChatCompletionRequest
	if err := json.NewDecoder(r.Body).Decode(&requestBody); err != nil {
		logError(w, "Error decoding request body: "+err.Error(), http.StatusBadRequest)
		return
	}

	// 构建请求体
	chatRequest := map[string]interface{}{
		"model": DefaultModel, // 请根据需要替换为正确的模型
		"messages": []map[string]string{
			{"role": "system", "content": "You are a helpful assistant."},
			{"role": "user", "content": requestBody.Messages[0].Content},
		},
		"stream": true,
	}
	requestBodyBytes, err := json.Marshal(chatRequest)
	if err != nil {
		logError(w, "Error encoding request body: "+err.Error(), http.StatusInternalServerError)
		return
	}

	// 构建HTTP请求
	req, err := http.NewRequest("POST", DefaultUrl, bytes.NewBuffer(requestBodyBytes))
	if err != nil {
		logError(w, "Error creating request: "+err.Error(), http.StatusInternalServerError)
		return
	}
	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", apiKey))

	// 发送请求
	client := &http.Client{}
	resp, err := client.Do(req)
	if err != nil {
		logError(w, "Error sending request: "+err.Error(), http.StatusInternalServerError)
		return
	}
	defer resp.Body.Close()

	// 设置Transfer-Encoding为chunked，这对于流式传输是必要的
	w.Header().Set("Transfer-Encoding", "chunked")

	// 创建一个缓冲区来存储从OpenAI接收到的数据
	buf := make([]byte, 1024)
	for {
		// 从OpenAI的响应中读取数据
		n, err := resp.Body.Read(buf)
		if n > 0 {
			// 将读取到的数据写入到客户端的响应中
			_, writeErr := w.Write(buf[:n])
			if writeErr != nil {
				// 如果写入客户端时出错，记录错误并退出循环
				log.Printf("Error writing to client: %v", writeErr)
				return
			}
			// 使用http.Flusher将缓冲中的数据立即发送给客户端
			if flusher, ok := w.(http.Flusher); ok {
				flusher.Flush()
			} else {
				log.Println("Expected http.ResponseWriter to be an http.Flusher")
				return
			}
		}
		if err == io.EOF {
			// 如果读取到的是EOF，表示OpenAI的响应已经结束
			break
		} else if err != nil {
			// 如果读取过程中出现错误，记录错误并退出循环
			log.Printf("Error reading from OpenAI response: %v", err)
			return
		}
	}
}

// func setOptionalFields(request *openai.ChatCompletionRequest, body CreateChatCompletionRequest) {
// 	if body.Model != "" {
// 		request.Model = body.Model
// 	}
// 	if body.Temperature != 0 {
// 		request.Temperature = float32(body.Temperature)
// 	}
// 	if body.MaxTokens != 0 {
// 		request.MaxTokens = int(body.MaxTokens)
// 	}
// 	if body.TopP != 0 {
// 		request.TopP = float32(body.TopP)
// 	}
// 	if body.FrequencyPenalty != 0 {
// 		request.FrequencyPenalty = float32(body.FrequencyPenalty)
// 	}
// 	if body.PresencePenalty != 0 {
// 		request.PresencePenalty = float32(body.PresencePenalty)
// 	}
// }
